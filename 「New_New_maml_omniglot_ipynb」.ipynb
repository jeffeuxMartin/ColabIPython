{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「New_New_maml_omniglot.ipynb」",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffeuxMartin/ColabIPython/blob/main/%E3%80%8CNew_New_maml_omniglot_ipynb%E3%80%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOTPvfBjwnhd"
      },
      "source": [
        "[Go_to_main](#mainprog)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME2dsUwWw_Dk",
        "outputId": "6ace187a-1919-4f51-a025-acbff1c46fea"
      },
      "source": [
        "import torch\n",
        "try:\n",
        "    # Get GPU name, check if it's K80\n",
        "    GPU_name = torch.cuda.get_device_name()\n",
        "    if GPU_name[-3:] == \"K80\":\n",
        "        print(\"Get K80! :'( RESTART!\")\n",
        "        exit()  # Restart the session\n",
        "    else:\n",
        "        print(\"Your GPU is {}!\".format(GPU_name))\n",
        "        print(\"Great! Keep going~\")\n",
        "except RuntimeError as e:\n",
        "    if e.args == (\"No CUDA GPUs are available\",):\n",
        "        print(\"You are training with CPU! \"\n",
        "              \"Please restart!\")\n",
        "        exit()  # Restart the session\n",
        "    else:\n",
        "        print(\"What's wrong here?\")\n",
        "        print(\"Error message: \\n\", e)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get K80! :'( RESTART!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tIp4RT5ugpa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a03de28-50a3-4242-dc2e-839a593f8161"
      },
      "source": [
        "!nvidia-smi\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 29 08:36:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7Gt4Jucug41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fde8cfe-86bc-48c8-cda2-bf5a3bf7454c"
      },
      "source": [
        "if 1:\n",
        "    workspace_dir = '.'\n",
        "\n",
        "    # gdown 是一個可以從 google drive 下載資料的工具\n",
        "    # gdown is a package that downloads files from       \\\n",
        "    #     google drive\n",
        "    !gdown --id 1FLDrQ0k-iJ-mk8ors0WItqvwgu0w9J0U \\\n",
        "        --output \"{workspace_dir}/Omniglot.tar.gz\"\n",
        "\n",
        "    # 使用 tar 解壓縮\n",
        "    # Use `tar' command to decompress\n",
        "    !tar -zxf \"{workspace_dir}/Omniglot.tar.gz\"          \\\n",
        "        -C \"{workspace_dir}/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FLDrQ0k-iJ-mk8ors0WItqvwgu0w9J0U\n",
            "To: /content/Omniglot.tar.gz\n",
            "26.4MB [00:00, 72.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvvlAQBUug42"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUK_cSRbug43"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YwMRiVwgtXF"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"      Created on Sat Apr 17 04:51:56 2021\n",
        "         @author: Jeff Chen                       \"\"\";"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUWvR4kpg2Bu"
      },
      "source": [
        "\"\"\" >>> Construct the Model \"\"\";"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9pfkqh8gxHD"
      },
      "source": [
        "# Import modules we need\n",
        "import glob\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgFbbKHYg3Hk"
      },
      "source": [
        "def ConvBlock(in_ch, out_ch):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "def ConvBlockFunction(x, w, b, w_bn, b_bn):\n",
        "    x = F.conv2d(x, w, b, padding=1)\n",
        "    x = F.batch_norm(x,\n",
        "                     running_mean=None,\n",
        "                     running_var=None,\n",
        "                     weight=w_bn, bias=b_bn,\n",
        "                     training=True)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, in_ch, k_way):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_ch, 64)\n",
        "        self.conv2 = ConvBlock(64, 64)\n",
        "        self.conv3 = ConvBlock(64, 64)\n",
        "        self.conv4 = ConvBlock(64, 64)\n",
        "        self.logits = nn.Linear(64, k_way)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.logits(x)\n",
        "        return x\n",
        "    def functional_forward(self, x, params):\n",
        "        '''\n",
        "        Arguments:\n",
        "        x: input images [batch, 1, 28, 28]\n",
        "        params: 模型的參數，也就是 convolution 的 weight\n",
        "                跟 bias，以及 batch normalization 的\n",
        "                weight 跟 bias\n",
        "                這是一個 OrderedDict\n",
        "        '''\n",
        "        for block in [1, 2, 3, 4]:\n",
        "            x = ConvBlockFunction(\n",
        "                x,\n",
        "                params[f'conv{block}.0.weight'],\n",
        "                params[f'conv{block}.0.bias'],\n",
        "                params.get(f'conv{block}.1.weight'),\n",
        "                params.get(f'conv{block}.1.bias'))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.linear(x,\n",
        "                     params['logits.weight'],\n",
        "                     params['logits.bias'])\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQF5vgLvg5aX"
      },
      "source": [
        "def create_label(n_way, k_shot):\n",
        "    return (torch.arange(n_way)\n",
        "                 .repeat_interleave(k_shot)\n",
        "                 .long())\n",
        "\n",
        "# 我們試著產生 5 way 2 shot 的 label 看看\n",
        "create_label(5, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0kNL95wg7Sr"
      },
      "source": [
        "def MAML(model, \n",
        "         optimizer, x, \n",
        "         n_way, k_shot, \n",
        "         q_query, \n",
        "         loss_fn, \n",
        "         inner_train_step=1, inner_lr=0.4, train=True\n",
        "   ): \n",
        "    \"\"\"\n",
        "    Args:\n",
        "    x is the input omniglot images for a meta_step, \n",
        "        shape = [batch_size, \n",
        "                 n_way * (k_shot + q_query), \n",
        "                 1, 28, 28]\n",
        "    n_way: 每個分類的 task 要有幾個 class\n",
        "    k_shot: 每個類別在 training 的時候會有多少張照片\n",
        "    q_query: 在 testing 時，每個類別會用多少張照片 update\n",
        "    \"\"\"\n",
        "\n",
        "    criterion = loss_fn\n",
        "    task_loss = []  # 這裡面之後會放入每個 task 的 loss\n",
        "    task_acc = []   # 這裡面之後會放入每個 task 的 acc\n",
        "\n",
        "    for meta_batch in x:\n",
        "        # support_set 是我們拿來 update inner loop \n",
        "        #    參數的 data\n",
        "        support_set = meta_batch[: n_way * k_shot]  \n",
        "        # query_set 是我們拿來 update outer loop \n",
        "        #    參數的 data\n",
        "        query_set = meta_batch[n_way * k_shot :]    \n",
        "        \n",
        "        # 在 inner loop update 參數時，我們不能動到實際\n",
        "        #    參數，因此用 fast_weights 來儲存新的參數 θ'\n",
        "        fast_weights = OrderedDict(\n",
        "                             model.named_parameters())\n",
        "        \n",
        "        for inner_step in range(inner_train_step): \n",
        "            train_label = create_label(\n",
        "                                 n_way, k_shot).cuda()\n",
        "            logits = model.functional_forward(\n",
        "                            support_set, fast_weights)\n",
        "            loss = criterion(logits, train_label)\n",
        "\n",
        "            # 這裡是要計算出 loss 對 θ 的微分 (∇loss)\n",
        "            grads = torch.autograd.grad(\n",
        "                loss, fast_weights.values(), \n",
        "                create_graph=True) \n",
        "            # 這裡是用剛剛算出的 ∇loss \n",
        "            #        來 update θ 變成 θ'\n",
        "            fast_weights = OrderedDict(\n",
        "                (name, param - inner_lr * grad)\n",
        "                for ((name, param), grad) in zip(\n",
        "                         fast_weights.items(), grads))\n",
        "   \n",
        "        val_label = create_label(\n",
        "                                n_way, q_query).cuda()\n",
        "\n",
        "        #$$ 一階微分，meta test\n",
        "        #<<<<<<<<<< -------- (a) --------- >>>>>>>>>>#\n",
        "        # 這裡用 query_set 和 θ' 算 logit              #\n",
        "        logits = model.functional_forward(           #\n",
        "                            query_set, fast_weights) #\n",
        "        #............................................#\n",
        "        # 這裡用 query_set 和 θ' 算 loss\n",
        "        loss = criterion(logits, val_label)\n",
        "        # 把這個 task 的 loss 丟進 task_loss 裡面\n",
        "        task_loss.append(loss)\n",
        "        # 算 accuracy\n",
        "        acc = np.asarray([(\n",
        "               torch.argmax(logits, -1).cpu().numpy()\n",
        "            == val_label.cpu().numpy())]).mean() \n",
        "        task_acc.append(acc)\n",
        "\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # 我們要用一整個 batch 的 loss 來 update θ (不是 θ')\n",
        "    meta_batch_loss = torch.stack(task_loss).mean()\n",
        "    if train:\n",
        "        meta_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "    task_acc = np.mean(task_acc)\n",
        "    return meta_batch_loss, task_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tJ2mot9hHPb"
      },
      "source": [
        "class Omniglot(Dataset):\n",
        "    def __init__(self, data_dir, k_way, q_query):\n",
        "        self.file_list = [f for f in glob.glob(\n",
        "            data_dir + \"**/character*\", \n",
        "            recursive=True)]\n",
        "        self.transform = transforms.Compose(\n",
        "                            [transforms.ToTensor()])\n",
        "        self.n = k_way + q_query\n",
        "    def __getitem__(self, idx):\n",
        "        sample = np.arange(20)\n",
        "        # 這裡是為了等一下要 random sample 出我們要的    \\\n",
        "        #     character\n",
        "        np.random.shuffle(sample) \n",
        "        img_path = self.file_list[idx]\n",
        "        img_list = [f for f in glob.glob(\n",
        "            img_path + \"**/*.png\", recursive=True)]\n",
        "        img_list.sort()\n",
        "        imgs = [self.transform(\n",
        "            Image.open(img_file)) \n",
        "            for img_file in img_list]\n",
        "        # 每個 character，取出 k_way + q_query 個\n",
        "        imgs = torch.stack(imgs)[sample[:self.n]] \n",
        "        return imgs\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoNhmWjMhMZR"
      },
      "source": [
        "\"\"\" >>> Start Training \"\"\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wFHmVcBhE4M"
      },
      "source": [
        "n_way = 5\n",
        "k_shot = 1\n",
        "q_query = 1\n",
        "inner_train_step = 1\n",
        "inner_lr = 0.4\n",
        "meta_lr = 0.001\n",
        "meta_batch_size = 32\n",
        "max_epoch = 80\n",
        "eval_batches = test_batches = 20\n",
        "train_data_path = './Omniglot/images_background/'\n",
        "test_data_path = './Omniglot/images_evaluation/'    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I13GJavhP0_"
      },
      "source": [
        "NUM_W = 2\n",
        "# dataset=Omniglot(train_data_path, k_shot, q_query)\n",
        "train_set, val_set = torch.utils.data.random_split(\n",
        "    Omniglot(train_data_path, k_shot, q_query),\n",
        "    [3200, 656])\n",
        "train_loader = DataLoader(train_set,\n",
        "                          # 這裡的 batch_size 並不是  \\\n",
        "                          #     meta batch size, 而  \\\n",
        "                          #     是一個 task 裡面會有多 \\\n",
        "                          #     少不同的 characters， \\\n",
        "                          #     也就是 few-shot      \\\n",
        "                          #     classification 的    \\\n",
        "                          #     n_way\n",
        "                          batch_size=n_way,\n",
        "                          num_workers=NUM_W,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True)\n",
        "val_loader = DataLoader(val_set,\n",
        "                        batch_size=n_way,\n",
        "                        num_workers=NUM_W,\n",
        "                        shuffle=True,\n",
        "                        drop_last=True)\n",
        "test_loader = DataLoader(Omniglot(\n",
        "                             test_data_path,\n",
        "                             k_shot, q_query),\n",
        "                         batch_size=n_way,\n",
        "                         num_workers=NUM_W,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "test_iter = iter(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxug882ihF2B"
      },
      "source": [
        "meta_model = Classifier(1, n_way).cuda()\n",
        "optimizer = torch.optim.Adam(meta_model.parameters(), \n",
        "                             lr=meta_lr)\n",
        "loss_fn = nn.CrossEntropyLoss().cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrkCSsxOhC-N"
      },
      "source": [
        "def get_meta_batch(meta_batch_size,\n",
        "                   k_shot, q_query, \n",
        "                   data_loader, iterator):\n",
        "    data = []\n",
        "    for _ in range(meta_batch_size):\n",
        "        try:\n",
        "            # 一筆 task_data 就是一個 task 裡面的 data，\\\n",
        "            #     大小是                              \\\n",
        "            #     [n_way, k_shot+q_query, 1, 28, 28]\n",
        "            task_data = iterator.next()  \n",
        "        except StopIteration:\n",
        "            iterator = iter(data_loader)\n",
        "            task_data = iterator.next()\n",
        "        train_data = (task_data[:, :k_shot]\n",
        "                      .reshape(-1, 1, 28, 28))\n",
        "        val_data = (task_data[:, k_shot:]\n",
        "                    .reshape(-1, 1, 28, 28))\n",
        "        task_data = torch.cat(\n",
        "            (train_data, val_data), 0)\n",
        "        data.append(task_data)\n",
        "    return torch.stack(data).cuda(), iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWQczA3FwjEG"
      },
      "source": [
        "<a name=\"mainprog\" id=\"mainprog\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQZjJrLAhBWw"
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "coriginalMAML = MAML\n",
        "# coriginalMAML = originalMAML\n",
        "for epoch in range(max_epoch):\n",
        "    print(\"Epoch %d\" % (epoch + 1))\n",
        "    train_meta_loss = []\n",
        "    train_acc = []\n",
        "    # 這裡的 step 是一次 meta-gradinet update step\n",
        "    for step in tqdm(range(\n",
        "            len(train_loader) // meta_batch_size)): \n",
        "        x, train_iter = get_meta_batch(\n",
        "            meta_batch_size, k_shot, q_query, \n",
        "            train_loader, train_iter)\n",
        "        meta_loss, acc = coriginalMAML(\n",
        "        # meta_loss, acc = MAML(\n",
        "            meta_model, optimizer, x, \n",
        "            n_way, k_shot, q_query, loss_fn)\n",
        "        train_meta_loss.append(meta_loss.item())\n",
        "        train_acc.append(acc)\n",
        "    print(\"  Loss    : \", \"%.3f\" % (np.mean(train_meta_loss)), end='\\t')\n",
        "    print(\"  Accuracy: \", \"%.3f %%\" % (np.mean(train_acc) * 100))\n",
        "\n",
        "    # 每個 epoch 結束後，看看 validation accuracy 如何  \n",
        "    # 助教並沒有做 early stopping，                  \\\n",
        "    #     同學如果覺得有需要是可以做的 \n",
        "    val_acc = []\n",
        "    for eval_step in tqdm(range(\n",
        "            len(val_loader) // (eval_batches))):\n",
        "        x, val_iter = get_meta_batch(\n",
        "            eval_batches, k_shot, q_query, \n",
        "            val_loader, val_iter)\n",
        "        # testing時，我們更新三次 inner-step\n",
        "        _, acc = coriginalMAML(meta_model, optimizer, x, \n",
        "        # _, acc = MAML(meta_model, optimizer, x, \n",
        "                      n_way, k_shot, q_query, \n",
        "                      loss_fn, \n",
        "                      inner_train_step=3, \n",
        "                      train=False) \n",
        "        val_acc.append(acc)\n",
        "    print(\"  Validation accuracy: \", \"%.3f %%\" % (np.mean(val_acc) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYN_zGB3g_5_"
      },
      "source": [
        "test_acc = []\n",
        "for test_step in tqdm(range(\n",
        "        len(test_loader) // (test_batches))):\n",
        "    x, test_iter = get_meta_batch(\n",
        "        test_batches, k_shot, q_query, \n",
        "        test_loader, test_iter)\n",
        "    # testing 時，我們更新三次 inner-step\n",
        "    _, acc = MAML(meta_model, optimizer, x, \n",
        "                  n_way, k_shot, q_query, loss_fn, \n",
        "                  inner_train_step=3, train=False)\n",
        "    test_acc.append(acc)\n",
        "print(\"  Testing accuracy: \", np.mean(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G--r4Y8ihTZ8"
      },
      "source": [
        "raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35yuMZZtB94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTutMHNshqtE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ypU5bnDyj9L"
      },
      "source": [
        "!mkdir /content/drive/MyDrive/Ml15/\n",
        "%cd /content/drive/MyDrive/Ml15/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILWDJriJzmsj"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWLGVXbyn-t"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z2otke-igEZ"
      },
      "source": [
        "workspace_dir = '.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p82jtNVWikJ9"
      },
      "source": [
        "# gdown 是一個可以從 google drive 下載資料的工具\n",
        "# gdown is a package that downloads files from       \\\n",
        "#     google drive\n",
        "!gdown --id 1FLDrQ0k-iJ-mk8ors0WItqvwgu0w9J0U \\\n",
        "       --output \"{workspace_dir}/Omniglot.tar.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvm8S51rhrJH"
      },
      "source": [
        "# 使用 tar 解壓縮\n",
        "# Use `tar' command to decompress\n",
        "!tar -zxf \"{workspace_dir}/Omniglot.tar.gz\"          \\\n",
        "     -C \"{workspace_dir}/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJjdfkqXvtGq"
      },
      "source": [
        "!rm -f Omniglot.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VJdBn1lxZ7w"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxCrg4MQxb83"
      },
      "source": [
        "rm -rf Omniglot/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb8Drjzbzs4J"
      },
      "source": [
        "%cd /content/drive/MyDrive/Ml15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld_74AHdzu5u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE49ozLQhrMV"
      },
      "source": [
        "from PIL import Image  # PIL 函式庫 / PIL library\n",
        "from IPython.display import display\n",
        "for i in range(10, 20):\n",
        "    im = Image.open(\n",
        "        \"Omniglot/images_background/\"\n",
        "        \"Japanese_(hiragana).0/\"\n",
        "        \"character13/0500_\" + str (i) + \".png\")\n",
        "    display(im)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}